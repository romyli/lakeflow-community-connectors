# ==============================================================================
# Merged Lakeflow Source: zoho_crm
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator, Optional
import json
import re
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from urllib.parse import quote
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/zoho_crm/zoho_crm.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Zoho CRM connector with connection-level options.

            Expected options:
                - client_id: OAuth Client ID from Zoho API Console
                - client_secret: OAuth Client Secret from Zoho API Console
                - refresh_token: Long-lived refresh token obtained from OAuth flow
                - base_url (optional): Zoho accounts URL for OAuth. Defaults to https://accounts.zoho.com
                  Examples: https://accounts.zoho.com (US), https://accounts.zoho.eu (EU),
                            https://accounts.zoho.in (IN), https://accounts.zoho.com.au (AU)
                - initial_load_start_date (optional): Starting point for the first sync. If omitted, syncs all historical data.

            Note: To obtain the refresh_token, follow the OAuth setup guide in
            sources/zoho_crm/configs/README.md or visit:
            https://www.zoho.com/accounts/protocol/oauth/web-apps/authorization.html
            """
            self.client_id = options.get("client_id")
            self.client_secret = options.get("client_secret")
            self.refresh_token = options.get("refresh_token")

            if not all([self.client_id, self.client_secret, self.refresh_token]):
                raise ValueError("Zoho CRM connector requires 'client_id', 'client_secret', and 'refresh_token' in options")

            # base_url is the accounts/OAuth URL (e.g., https://accounts.zoho.eu)
            self.accounts_url = options.get("base_url", "https://accounts.zoho.com").rstrip("/")

            # Derive the API URL from the accounts URL by extracting the domain suffix
            # https://accounts.zoho.eu -> https://www.zohoapis.eu
            # https://accounts.zoho.com -> https://www.zohoapis.com
            match = re.search(r"accounts\.zoho\.(.+)$", self.accounts_url)
            domain_suffix = match.group(1) if match else "com"
            self.api_url = f"https://www.zohoapis.{domain_suffix}"

            self.initial_load_start_date = options.get("initial_load_start_date")

            # OAuth token management
            self.access_token = None
            self.token_expires_at = None

            # Configure a session for API requests
            self._session = requests.Session()

            # Cache for module and field metadata
            self._modules_cache = None
            self._fields_cache = {}

            # Track maximum Modified_Time seen during read operations
            self._current_max_modified_time = None

        def _get_access_token(self) -> str:
            """
            Get a valid access token, refreshing if necessary.
            Access tokens expire after 1 hour (3600 seconds).
            """
            # Check if we have a valid token
            if self.access_token and self.token_expires_at:
                if datetime.now() < self.token_expires_at - timedelta(minutes=5):
                    return self.access_token

            # Refresh the access token using the accounts URL
            token_url = f"{self.accounts_url}/oauth/v2/token"

            data = {
                "refresh_token": self.refresh_token,
                "client_id": self.client_id,
                "client_secret": self.client_secret,
                "grant_type": "refresh_token",
            }

            response = requests.post(token_url, data=data)

            try:
                response.raise_for_status()
            except requests.exceptions.HTTPError as e:
                error_detail = response.text
                raise Exception(f"Failed to refresh access token: {e}. Response: {error_detail}")

            token_data = response.json()

            if "access_token" not in token_data:
                raise Exception(
                    f"Token refresh response missing 'access_token'. "
                    f"Response: {token_data}. "
                    f"Please check your client_id, client_secret, and refresh_token are valid."
                )

            self.access_token = token_data["access_token"]
            expires_in = token_data.get("expires_in", 3600)
            self.token_expires_at = datetime.now() + timedelta(seconds=expires_in)

            return self.access_token

        def _make_request(
            self,
            method: str,
            endpoint: str,
            params: Optional[dict] = None,
            data: Optional[dict] = None,
        ) -> dict:
            """
            Make an authenticated API request to Zoho CRM.
            Handles token refresh and rate limiting.
            """
            access_token = self._get_access_token()

            url = f"{self.api_url}{endpoint}"
            headers = {
                "Authorization": f"Zoho-oauthtoken {access_token}",
            }

            if data:
                headers["Content-Type"] = "application/json"

            max_retries = 3
            for attempt in range(max_retries):
                try:
                    if method.upper() == "GET":
                        response = self._session.get(url, headers=headers, params=params)
                    elif method.upper() == "POST":
                        response = self._session.post(url, headers=headers, json=data, params=params)
                    elif method.upper() == "PUT":
                        response = self._session.put(url, headers=headers, json=data, params=params)
                    elif method.upper() == "DELETE":
                        response = self._session.delete(url, headers=headers, params=params)
                    else:
                        raise ValueError(f"Unsupported HTTP method: {method}")

                    # Handle rate limiting
                    if response.status_code == 429:
                        if attempt < max_retries - 1:
                            # Exponential backoff: 2^attempt seconds
                            wait_time = 2**attempt
                            time.sleep(wait_time)
                            continue
                        else:
                            raise Exception("Rate limit exceeded. Please wait and try again later.")

                    response.raise_for_status()

                    # Handle empty responses (some Zoho modules return empty body)
                    if not response.text or response.text.strip() == "":
                        return {}

                    return response.json()

                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 401:
                        # Token might be invalid, try refreshing once
                        if attempt == 0:
                            self.access_token = None
                            access_token = self._get_access_token()
                            headers["Authorization"] = f"Zoho-oauthtoken {access_token}"
                            continue
                    raise

            raise Exception(f"Failed to make request after {max_retries} attempts")

        def _get_modules(self) -> list[dict]:
            """
            Retrieve all available modules from Zoho CRM.
            Results are cached to avoid repeated API calls.
            """
            if self._modules_cache is not None:
                return self._modules_cache

            response = self._make_request("GET", "/crm/v8/settings/modules")
            modules = response.get("modules", [])

            # Filter for API-supported modules (default or custom types)
            supported_modules = [m for m in modules if m.get("api_supported") and m.get("generated_type") in ["default", "custom"]]

            self._modules_cache = supported_modules
            return supported_modules

        def _get_fields(self, module_name: str) -> list[dict]:
            """
            Retrieve field metadata for a specific module.
            Results are cached per module.
            """
            if module_name in self._fields_cache:
                return self._fields_cache[module_name]

            params = {"module": module_name}
            try:
                response = self._make_request("GET", "/crm/v8/settings/fields", params=params)
                fields = response.get("fields", [])
            except Exception as e:
                # Some modules may not support fields API or return empty responses
                print(f"Warning: Could not fetch fields for {module_name}: {e}")
                fields = []

            self._fields_cache[module_name] = fields
            return fields

        def list_tables(self) -> list[str]:
            """
            List names of all tables (modules) supported by this connector.
            Uses the Modules API to dynamically discover available modules.
            """
            modules = self._get_modules()
            return [m["api_name"] for m in modules]

        def _zoho_type_to_spark_type(self, field: dict) -> StructField:
            """
            Convert a Zoho CRM field definition to a Spark StructField.
            """
            api_name = field["api_name"]
            data_type = field.get("data_type", "text")
            json_type = field.get("json_type")
            nullable = not field.get("required", False)

            # Map Zoho data types to Spark types
            if data_type == "bigint":
                spark_type = LongType()
            elif data_type in ["text", "textarea", "email", "phone", "website", "autonumber"]:
                spark_type = StringType()
            elif data_type == "picklist":
                spark_type = StringType()
            elif data_type == "multiselectpicklist":
                spark_type = ArrayType(StringType(), True)
            elif data_type == "integer":
                spark_type = LongType()  # Prefer LongType over IntegerType
            elif data_type in ["double", "currency", "percent"]:
                spark_type = DoubleType()
            elif data_type == "boolean":
                spark_type = BooleanType()
            elif data_type in ["date", "datetime"]:
                spark_type = StringType()  # Store as ISO 8601 string
            elif data_type in ["lookup", "ownerlookup"]:
                # Lookup fields are nested objects
                lookup_struct = StructType(
                    [
                        StructField("id", StringType(), True),
                        StructField("name", StringType(), True),
                        StructField("email", StringType(), True),
                    ]
                )
                spark_type = lookup_struct
            elif data_type == "multiselectlookup":
                lookup_struct = StructType(
                    [
                        StructField("id", StringType(), True),
                        StructField("name", StringType(), True),
                    ]
                )
                spark_type = ArrayType(lookup_struct, True)
            elif data_type in ["fileupload", "imageupload", "profileimage"]:
                spark_type = StringType()
            elif data_type == "subform":
                # Subforms are arrays of nested objects
                # We'll use a flexible structure since subform schemas vary
                spark_type = ArrayType(
                    StructType(
                        [
                            StructField("id", StringType(), True),
                        ]
                    ),
                    True,
                )
            elif data_type == "consent_lookup":
                spark_type = StructType(
                    [
                        StructField("id", StringType(), True),
                        StructField("name", StringType(), True),
                    ]
                )
            elif data_type == "event_reminder":
                spark_type = StringType()
            elif data_type == "RRULE":
                spark_type = StructType(
                    [
                        StructField("FREQ", StringType(), True),
                        StructField("INTERVAL", StringType(), True),
                    ]
                )
            elif data_type == "ALARM":
                spark_type = StructType(
                    [
                        StructField("ACTION", StringType(), True),
                    ]
                )
            else:
                # Default to StringType for unknown types
                spark_type = StringType()

            # Handle special JSON types - store as JSON strings for flexibility
            if json_type == "jsonarray":
                spark_type = StringType()  # Store as JSON string
            elif json_type == "jsonobject":
                spark_type = StringType()  # Store as JSON string

            return StructField(api_name, spark_type, nullable)

        def get_table_schema(self, table_name: str, table_options: dict[str, str]) -> StructType:
            """
            Fetch the schema of a module dynamically from Zoho CRM.
            Uses the Fields Metadata API to build the schema.
            """
            # Check if table exists
            available_tables = self.list_tables()
            if table_name not in available_tables:
                raise ValueError(f"Table '{table_name}' is not supported. " f"Available tables: {', '.join(available_tables)}")

            # Get field metadata for this module
            fields = self._get_fields(table_name)

            # If no fields are available, return a minimal schema with just id
            if not fields:
                print(f"Warning: No fields available for {table_name}, using minimal schema")
                return StructType([StructField("id", LongType(), False)])

            # Convert Zoho fields to Spark schema
            struct_fields = []
            for field in fields:
                try:
                    struct_field = self._zoho_type_to_spark_type(field)
                    struct_fields.append(struct_field)
                except Exception as e:
                    # Log warning and skip problematic fields
                    print(f"Warning: Could not convert field {field.get('api_name')}: {e}")
                    continue

            return StructType(struct_fields)

        def read_table_metadata(self, table_name: str, table_options: dict[str, str]) -> dict:
            """
            Fetch the metadata of a module.

            For Zoho CRM:
            - Primary key is always 'id'
            - Cursor field is 'Modified_Time' for CDC
            - Most modules support CDC ingestion
            """
            # Check if table exists
            available_tables = self.list_tables()
            if table_name not in available_tables:
                raise ValueError(f"Table '{table_name}' is not supported. " f"Available tables: {', '.join(available_tables)}")

            # Get the schema to check if Modified_Time exists
            schema = self.get_table_schema(table_name, table_options)
            field_names = schema.fieldNames()
            has_modified_time = "Modified_Time" in field_names
            has_id = "id" in field_names

            # Attachments are append-only
            if table_name == "Attachments":
                return {
                    "primary_keys": ["id"] if has_id else [],
                    "ingestion_type": "append",
                }

            # Analytics and special modules without Modified_Time use snapshot
            if not has_modified_time:
                return {
                    "primary_keys": ["id"] if has_id else [],
                    "ingestion_type": "snapshot",
                }

            # All other modules support CDC with Modified_Time cursor
            return {
                "primary_keys": ["id"],
                "cursor_field": "Modified_Time",
                "ingestion_type": "cdc",
            }

        def read_table(self, table_name: str, start_offset: dict, table_options: dict[str, str]) -> (Iterator[dict], dict):
            """
            Read records from a Zoho CRM module.
            Supports incremental reads using Modified_Time cursor and pagination.
            Also fetches deleted records for CDC.
            """
            print(f"[DEBUG] read_table called for '{table_name}'")
            print(f"[DEBUG] start_offset: {start_offset}")
            print(f"[DEBUG] initial_load_start_date: {self.initial_load_start_date}")

            # Check if table exists
            available_tables = self.list_tables()
            if table_name not in available_tables:
                raise ValueError(f"Table '{table_name}' is not supported. " f"Available tables: {', '.join(available_tables)}")

            # Determine the cursor time for incremental reads
            cursor_time = None
            if start_offset and "cursor_time" in start_offset:
                cursor_time = start_offset["cursor_time"]
                print(f"[DEBUG] Using cursor_time from start_offset: {cursor_time}")
            elif self.initial_load_start_date:
                cursor_time = self.initial_load_start_date
                print(f"[DEBUG] Using cursor_time from initial_load_start_date: {cursor_time}")
            else:
                print("[DEBUG] No cursor_time - will fetch all records")

            # Apply a 5-minute lookback window to catch late updates
            if cursor_time:
                cursor_dt = datetime.fromisoformat(cursor_time.replace("Z", "+00:00"))
                lookback_dt = cursor_dt - timedelta(minutes=5)
                cursor_time = lookback_dt.strftime("%Y-%m-%dT%H:%M:%S+00:00")
                print(f"[DEBUG] cursor_time after lookback adjustment: {cursor_time}")

            # Get metadata to determine ingestion type
            metadata = self.read_table_metadata(table_name, table_options)
            ingestion_type = metadata.get("ingestion_type")

            # For snapshot tables, don't use cursor_time (they don't support filtering)
            if ingestion_type == "snapshot":
                cursor_time = None

            # Fetch regular records
            records_iter = self._read_records(table_name, cursor_time)

            # For CDC ingestion, also fetch deleted records
            if ingestion_type == "cdc" and cursor_time:
                # Collect all records first to avoid generator re-entrance issues
                all_records = list(records_iter)
                deleted_records = list(self._read_deleted_records(table_name, cursor_time))

                # Combine all records
                def combined_iter():
                    yield from all_records
                    yield from deleted_records

                records_iter = combined_iter()

            # Calculate the next offset based on the maximum Modified_Time seen
            max_modified_time = self._current_max_modified_time
            if max_modified_time:
                next_offset = {"cursor_time": max_modified_time}
            else:
                # No records found, keep the same offset
                next_offset = start_offset or {}

            print(f"[DEBUG] read_table returning. next_offset: {next_offset}")
            return records_iter, next_offset

        def _get_json_fields(self, module_name: str) -> set:
            """
            Get field names that should be serialized as JSON strings.
            These are fields with json_type 'jsonobject' or 'jsonarray'.
            """
            cache_key = f"{module_name}_json_fields"
            if cache_key in self._fields_cache:
                return self._fields_cache[cache_key]

            fields = self._get_fields(module_name)
            json_fields = set()
            for field in fields:
                json_type = field.get("json_type")
                if json_type in ("jsonobject", "jsonarray"):
                    json_fields.add(field.get("api_name"))

            self._fields_cache[cache_key] = json_fields
            return json_fields

        def _normalize_record(self, record: dict, json_fields: set) -> dict:
            """
            Normalize a record for Spark compatibility.
            Only serializes fields that are declared as JSON strings in schema.
            """
            normalized = {}
            for key, value in record.items():
                if value is None:
                    normalized[key] = None
                elif key in json_fields and isinstance(value, (dict, list)):
                    # Only serialize fields that are declared as StringType for JSON
                    normalized[key] = json.dumps(value)
                else:
                    normalized[key] = value
            return normalized

        def _read_records(self, module_name: str, cursor_time: Optional[str] = None) -> Iterator[dict]:
            """
            Read records from a module with pagination.
            """
            print(f"[DEBUG] _read_records called for '{module_name}' with cursor_time: {cursor_time}")
            self._current_max_modified_time = cursor_time

            # Get fields that need JSON serialization
            json_fields = self._get_json_fields(module_name)
            print(f"[DEBUG] JSON fields to serialize: {json_fields}")

            page = 1
            per_page = 200  # Maximum allowed by Zoho CRM
            total_records_yielded = 0

            while True:
                params = {
                    "page": page,
                    "per_page": per_page,
                    "sort_order": "asc",
                    "sort_by": "Modified_Time",
                }

                # Add incremental filter if cursor_time is provided
                if cursor_time:
                    # URL encode the criteria
                    criteria = f"(Modified_Time:greater_equal:{cursor_time})"
                    params["criteria"] = criteria

                print(f"[DEBUG] API request: GET /crm/v8/{module_name} with params: {params}")

                try:
                    response = self._make_request("GET", f"/crm/v8/{module_name}", params=params)
                    print(f"[DEBUG] API response keys: {response.keys() if response else 'None'}")
                except Exception as e:
                    print(f"[DEBUG] ERROR fetching page {page} for {module_name}: {e}")
                    raise

                data = response.get("data", [])
                info = response.get("info", {})
                print(f"[DEBUG] Page {page}: got {len(data)} records, info: {info}")

                # Track the maximum Modified_Time seen
                for record in data:
                    modified_time = record.get("Modified_Time")
                    if modified_time:
                        if not self._current_max_modified_time or modified_time > self._current_max_modified_time:
                            self._current_max_modified_time = modified_time

                    total_records_yielded += 1
                    yield self._normalize_record(record, json_fields)

                # Check if there are more pages
                more_records = info.get("more_records", False)
                print(f"[DEBUG] more_records: {more_records}, data empty: {len(data) == 0}")
                if not more_records or not data:
                    print(f"[DEBUG] Stopping pagination. Total records yielded: {total_records_yielded}")
                    break

                page += 1

        def _read_deleted_records(self, module_name: str, cursor_time: Optional[str] = None) -> Iterator[dict]:
            """
            Read deleted records from a module.
            Returns records marked for deletion with a special field.
            """
            page = 1
            per_page = 200

            while True:
                params = {
                    "type": "all",
                    "page": page,
                    "per_page": per_page,
                }

                try:
                    response = self._make_request("GET", f"/crm/v8/{module_name}/deleted", params=params)
                except Exception as e:
                    print(f"[DEBUG] ERROR fetching deleted records for {module_name}: {e}")
                    raise

                data = response.get("data", [])
                info = response.get("info", {})

                # Filter by cursor_time if provided
                for record in data:
                    deleted_time = record.get("deleted_time")

                    # Only include records deleted after cursor_time
                    if cursor_time and deleted_time:
                        if deleted_time < cursor_time:
                            continue

                    # Mark this record as deleted
                    record["_zoho_deleted"] = True

                    # Update max modified time to deleted_time
                    if deleted_time:
                        if not self._current_max_modified_time or deleted_time > self._current_max_modified_time:
                            self._current_max_modified_time = deleted_time

                    yield record

                # Check if there are more pages
                more_records = info.get("more_records", False)
                if not more_records or not data:
                    break

                page += 1


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
