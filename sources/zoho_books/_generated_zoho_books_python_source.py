# ==============================================================================
# Merged Lakeflow Source: zoho_books
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Iterator, Union
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/zoho_books/zoho_books.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the source connector with parameters needed to connect to the source.
            Args:
                options: A dictionary of parameters like authentication tokens, table names, and other configurations.
                         Expected options:
                         - access_token: OAuth Access Token
                         - access_token_expires_at: ISO formatted string of when the access token expires (e.g., "YYYY-MM-DDTHH:MM:SS.ffffffZ")
                         - organization_id: Zoho Books Organization ID
                         - region: Zoho Books API region (e.g., 'com', 'eu', 'in')
            """
            self.access_token = options.get("access_token")
            self.access_token_expires_at_str = options.get("access_token_expires_at")
            self.organization_id = options.get("organization_id")
            self.region = options.get("region", "com") # Default to .com if not specified

            if not all([self.access_token, self.access_token_expires_at_str, self.organization_id]):
                raise ValueError("Missing required authentication or organization parameters.")

            try:
                # Parse the expiry time string into a datetime object
                self.access_token_expires_at = datetime.fromisoformat(self.access_token_expires_at_str.replace('Z', '+00:00'))
            except ValueError:
                raise ValueError(f"Invalid access_token_expires_at format: {self.access_token_expires_at_str}. Expected ISO format.")

            self.base_api_uri = f"https://www.zohoapis.{self.region}/books/v3"

            # Supported tables from zoho_books_api_doc.md
            self.supported_tables = [
                "Organizations", "Contacts", "Contact Persons", "Estimates", "Sales Orders",
                "Sales Receipts", "Invoices", "Recurring Invoices", "Credit Notes",
                "Customer Debit Notes", "Customer Payments", "Expenses", "Recurring Expenses",
                "Bills", "Vendor Credits", "Purchase Orders", "Recurring Bills", "Vendors",
                "Chart of Accounts", "Bank Accounts", "Bank Rules", "Transaction Categories",
                "Currencies", "Taxes", "Items", "Locations", "Opening Balance", "Users",
                "Zoho CRM Integration" # Note: This is an integration, likely not a direct table for data pull
            ]

        def _get_valid_access_token(self) -> str:
            """
            Returns a valid access token. This method assumes the provided access_token is always valid and
            refreshed externally before being passed to the connector.
            """
            if datetime.now() >= self.access_token_expires_at:
                raise ValueError("Access token has expired. It must be refreshed externally.")
            return self.access_token

        def _make_api_call(
            self, endpoint: str, params: dict = None, method: str = "GET"
        ) -> dict:
            """
            Makes an authenticated API call to Zoho Books.
            Args:
                endpoint: The API endpoint relative to the base URI (e.g., "/contacts").
                params: Optional dictionary of query parameters.
                method: HTTP method (GET, POST, PUT).
            Returns:
                The JSON response from the API.
            """
            access_token = self._get_valid_access_token()
            headers = {"Authorization": f"Zoho-oauthtoken {access_token}"}
            url = f"{self.base_api_uri}{endpoint}"

            if params is None:
                params = {}
            params["organization_id"] = self.organization_id

            if method == "GET":
                response = requests.get(url, headers=headers, params=params)
            elif method == "POST":
                response = requests.post(url, headers=headers, json=params) # Assuming JSON body for POST/PUT
            elif method == "PUT":
                response = requests.put(url, headers=headers, json=params)
            else:
                raise ValueError(f"Unsupported HTTP method: {method}")

            response.raise_for_status() # Raise an exception for HTTP errors
            return response.json()

        def list_tables(self) -> list[str]:
            """
            List names of all the tables supported by the source connector.
            Returns:
                A list of table names.
            """
            return self.supported_tables

        def _infer_spark_type(self, value: Any) -> StructType | ArrayType | StringType | LongType | DoubleType | BooleanType | TimestampType | DateType:
            """
            Infers the Spark SQL type for a given Python value.
            Prefers LongType over IntegerType.
            """
            if isinstance(value, str):
                # Try to parse as datetime for TimestampType or DateType
                try:
                    datetime.fromisoformat(value.replace('Z', '+00:00')) # Handles 'Z' for UTC
                    return TimestampType()
                except ValueError:
                    pass
                try:
                    datetime.strptime(value, "%Y-%m-%d")
                    return DateType()
                except ValueError:
                    pass
                return StringType()
            elif isinstance(value, bool):
                return BooleanType()
            elif isinstance(value, int):
                return LongType() # Prefer LongType over IntegerType
            elif isinstance(value, float):
                return DoubleType()
            elif isinstance(value, list):
                if not value:
                    return ArrayType(StringType()) # Default to StringType for empty lists
                # Infer type from the first element
                first_element_type = self._infer_spark_type(value[0])
                # If it's a StructField (which it shouldn't be, _infer_spark_type returns dataType directly now)
                # or a StructType, extract its dataType or use directly
                if isinstance(first_element_type, StructType):
                    return ArrayType(first_element_type)
                return ArrayType(first_element_type)
            elif isinstance(value, dict):
                # Recursively build schema for nested dictionaries
                return self._build_struct_type_from_json(value)
            elif value is None:
                return StringType() # Default to StringType for None, schema inference is tricky with None
            else:
                return StringType() # Catch-all for unknown types

        def _build_struct_type_from_json(self, json_data: dict) -> StructType:
            """
            Builds a StructType schema from a dictionary of JSON data.
            Avoids flattening nested fields.
            """
            fields = []
            for key, value in json_data.items():
                field_type = self._infer_spark_type(value)
                fields.append(StructField(key, field_type, True))
            return StructType(fields)


        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            Args:
                table_name: The name of the table to fetch the schema for.
                table_options: A dictionary of options for accessing the table.
            Returns:
                A StructType object representing the schema of the table.
            """
            if table_name not in self.supported_tables:
                raise ValueError(f"Table '{table_name}' is not supported.")

            # Special handling for "Organizations" as it's a top-level endpoint
            if table_name == "Organizations":
                endpoint = "/organizations"
            else:
                # Most tables follow a pluralized endpoint name
                endpoint = f"/{table_name.lower().replace(' ', '')}s" # Basic pluralization

            # Attempt to fetch a single record or a list to infer schema
            try:
                response_data = self._make_api_call(endpoint, params={"per_page": 1})
                if table_name == "Organizations":
                    records = response_data.get("organizations", [])
                else:
                    # Assuming the key for the list of records is the lowercase plural of the table name
                    key = table_name.lower().replace(' ', '') + "s"
                    records = response_data.get(key, [])

                if records:
                    # Use the first record to infer the schema
                    return self._build_struct_type_from_json(records[0])
                else:
                    # If no records, try to fetch a single object if a 'get' endpoint exists and infer from there
                    # This is a simplification; a more robust solution would know the singular endpoint for each object
                    # For now, return an empty StructType if no data to infer from
                    return StructType([])
            except Exception as e:
                # Handle API errors or no data found
                print(f"Error fetching data for schema inference for table {table_name}: {e}")
                return StructType([]) # Return empty schema on error or no data

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            Args:
                table_name: The name of the table to fetch the metadata for.
                table_options: A dictionary of options for accessing the table.
            Returns:
                A dictionary containing the metadata of the table.
            """
            if table_name not in self.supported_tables:
                raise ValueError(f"Table '{table_name}' is not supported.")

            primary_keys = []
            cursor_field = None
            ingestion_type = "snapshot" # Default ingestion type

            # Primary Keys (from zoho_books_api_doc.md)
            if table_name == "Organizations":
                primary_keys = ["organization_id"]
            elif table_name == "Contacts":
                primary_keys = ["contact_id"]
            elif table_name == "Invoices":
                primary_keys = ["invoice_id"]
            elif table_name == "Estimates":
                primary_keys = ["estimate_id"]
            elif table_name == "Sales Orders":
                primary_keys = ["salesorder_id"]
            elif table_name == "Users":
                primary_keys = ["user_id"]
            # Add more primary keys for other tables as needed based on API docs

            # Ingestion Types and Cursor Fields (from zoho_books_api_doc.md, Fivetran, Airbyte)
            cdc_tables = [
                "BILL", "CHART_OF_ACCOUNT", "CREDIT_NOTE_REFUND", "CURRENCY", "ESTIMATE",
                "EXPENSE", "ITEM", "JOURNAL_LIST", "PROJECT", "PURCHASE_ORDER",
                "SALES_ORDER", "TAX", "TIME_ENTRY", # Fivetran lists these for deletes, implying CDC
            ]
            append_tables = [
                "Contacts", "Credit Notes", "Customer Payments", "Invoices" # Fivetran lists these for incremental new records
            ]

            # Normalize table names for comparison with the doc's lists which are capitalized
            normalized_table_name = table_name.replace(' ', '_').upper()

            if normalized_table_name in [t.replace(' ', '_').upper() for t in cdc_tables]:
                ingestion_type = "cdc"
                # Common cursor fields for CDC
                cursor_field = "last_modified_time" # Assuming common pattern
                if not primary_keys: # If primary_keys were not set above, it means no explicit PK in docs
                    raise ValueError(f"Primary keys are required for CDC table '{table_name}'.")
            elif normalized_table_name in [t.replace(' ', '_').upper() for t in append_tables]:
                ingestion_type = "append"
                # Common cursor fields for append
                cursor_field = "created_time" # Assuming common pattern
                if not primary_keys: # If primary_keys were not set above, it means no explicit PK in docs
                    raise ValueError(f"Primary keys are required for append table '{table_name}'.")
            elif table_name == "Users":
                ingestion_type = "snapshot" # Airbyte specifies full sync only
                # primary_keys already set above
            # TBD: For other tables not explicitly categorized, it defaults to snapshot.
            # If primary keys are crucial for snapshot tables, they should be defined.

            return {
                "primary_keys": primary_keys,
                "cursor_field": cursor_field,
                "ingestion_type": ingestion_type,
            }

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the records of a table and return an iterator of records and an offset.
            The read starts from the provided start_offset.
            Records returned in the iterator will be one batch of records marked by the offset as its end_offset.
            The read_table function could be called multiple times to read the entire table in multiple batches and
            it stops when the same offset is returned again.
            If the table cannot be incrementally read, the offset can be None if we want to read the entire table in one batch.
            We could still return some fake offsets (cannot checkpointing) to split the table into multiple batches.
            Args:
                table_name: The name of the table to read.
                start_offset: The offset to start reading from.
                table_options: A dictionary of options for accessing the table.
            Returns:
                An iterator of records in JSON format and an offset.
            """
            if table_name not in self.supported_tables:
                raise ValueError(f"Table '{table_name}' is not supported.")

            metadata = self.read_table_metadata(table_name, table_options)
            ingestion_type = metadata.get("ingestion_type")
            cursor_field = metadata.get("cursor_field")

            current_page = int(start_offset.get("page", 1)) if start_offset else 1
            records_per_page = 200 # A reasonable default, Zoho API might have its own default or max.

            params = {"per_page": records_per_page, "page": current_page}

            if ingestion_type in ["cdc", "append"] and cursor_field and start_offset and start_offset.get(cursor_field):
                # For incremental reads, filter by cursor_field if available in start_offset
                # Zoho Books API doesn't seem to have a generic `created_time_gte` or `last_modified_time_gte`
                # for all objects in the v3 docs. This is a potential limitation.
                # For now, we'll assume pagination is the primary mechanism and this will handle "new records".
                # True incremental (e.g., CDC based on timestamps) would require more specific API features.
                pass # Placeholder, as direct timestamp filtering might not be universally supported for incremental in Zoho Books API v3 without explicit API calls per object

            # Special handling for "Organizations"
            if table_name == "Organizations":
                endpoint = "/organizations"
                response_key = "organizations"
            else:
                endpoint = f"/{table_name.lower().replace(' ', '')}s"
                response_key = table_name.lower().replace(' ', '') + "s"

            try:
                response_data = self._make_api_call(endpoint, params=params)
                records = response_data.get(response_key, [])
                page_context = response_data.get("page_context", {})
                has_more_pages = page_context.get("has_more_page", False)

                if records:
                    next_offset = {"page": current_page + 1} if has_more_pages else None
                    return iter(records), next_offset
                else:
                    return iter([]), None # No more records
            except Exception as e:
                print(f"Error reading table {table_name}: {e}")
                return iter([]), None # Return empty iterator on error


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
